slices:
  - sources:
      - model: mistralai/Mistral-7B-Instruct-v0.2
        layer_range: [0, 40]
      - model: Nexusflow/Starling-LM-7B-beta
        layer_range: [0, 40]
# models: 2 models to merge, MIXTRAL and LLAMA
#   - model: mistralai/Mistral-7B-Instruct-v0.2
#   - model: Nexusflow/Starling-LM-7B-beta
# layer_range: [0, 40] for both models, to merge first 40 layers

# Slerp is Spherical Linear Interpolation for smooth transition between models
# base_model: mistralai/Mistral-7B-Instruct-v0.2
# merge_method: slerp
# parameters: t for interpolation between models
# Self-attention weights are interpolated between 0 and 1
#   - filter: self_attn
# Value of MLP weights are interpolated between 1 and 0
#     value: [0, 0.5, 0.3, 0.7, 1]

merge_method: slerp
base_model: mistralai/Mistral-7B-Instruct-v0.2
parameters:
  t:
    - filter: self_attn
      value: [0, 0.5, 0.3, 0.7, 1]
    - filter: mlp
      value: [1, 0.5, 0.7, 0.3, 0]
    - value: 0.5 # fallback for rest of tensors
dtype: float16
# dtype: float16 for faster inference and less memory usage 